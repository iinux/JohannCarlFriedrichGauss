

# docker-compose

```sh
docker-compose -f wp-compose.yml up -d
docker-compose ps
docker-compose -f wp-compose.yml exec -it nginx sh

# ping mariadb 和 wordpress 这两个服务，网络都是通的，不过它的 IP 地址段用的是“172.22.0.0/16”，和 Docker 默认的“172.17.0.0/16”不一样。
# docker-compose 的命令与 Docker 类似，比较常用的有 up、ps、down，用来启动、查看和停止应用。
# 默认名字是 composer.yml 如果是这个名字，就不用加 -f 了
```

# PersistentVolume

“accessModes”定义了存储设备的访问模式，简单来说就是虚拟盘的读写权限，和 Linux 的文件访问模式差不多，目前 Kubernetes 里有 3 种： 
* ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。
* ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。 
* ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。

Kubernetes 里定义存储容量使用的是国际标准，我们日常习惯使用的 KB/MB/GB 的基数是 1024，要写成 Ki/Mi/Gi

kubectl get pv
kubectl get pvc

## NFS

```sh
sudo apt -y install nfs-kernel-server
sudo yum -y install nfs-utils rpcbind 
mkdir -p /tmp/nfs

`/etc/exports`

/tmp/nfs 10.0.2.0/24(rw,sync,no_subtree_check,no_root_squash,insecure)

sudo exportfs -ra
sudo exportfs -v

sudo systemctl start  nfs-server
sudo systemctl enable nfs-server
sudo systemctl status nfs-server

showmount -e 127.0.0.1

# 有了 NFS 服务器之后，为了让 Kubernetes 集群能够访问 NFS 存储服务，我们还需要在每个节点上都安装 NFS 客户端。
sudo apt -y install nfs-common

showmount -e 10.0.2.4
mkdir -p /tmp/test
sudo mount -t nfs 10.0.2.4:/tmp/nfs /tmp/test


```

## dynamic

* 这个在 Kubernetes 里就是“动态存储卷”的概念，它可以用 StorageClass 绑定一个 Provisioner 对象，
* 而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。 
* 有了“动态存储卷”的概念，前面我们讲的手工创建的 PV 就可以称为“静态存储卷”。

https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

* NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。

* 不过这三个文件只是示例，想在我们的集群里真正运行起来还要修改其中的两个文件。 
* 第一个要修改的是 rbac.yaml，它使用的是默认的 default 名字空间，应该把它改成其他的名字空间，避免与普通应用混在一起，你可以用“查找替换”的方式把它统一改成 kube-system。 
* 第二个要修改的是 deployment.yaml，它要修改的地方比较多。首先要把名字空间改成和 rbac.yaml 一样，比如是 kube-system，然后重点要修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。

provisioner.go:247] Error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

# StatefulSet

* 启动顺序
* 依赖关系
* 网络标识

```sh
kubectl get sts

kubectl exec -it redis-sts-0 -- sh

echo $HOSTNAME
hostname
```

* 名字顺序
* 启动顺序

# rollout

```sh
kubectl rollout status deployment ngx-dep
kubectl rollout pause
kubectl rollout resume
kubectl rollout history deploy ngx-dep
kubectl rollout history deploy ngx-dep  --revision=2
kubectl rollout undo deploy ngx-dep
```

如果用一个简单的比喻来说呢，annotations 就是包装盒里的产品说明书，而 labels 是包装盒外的标签贴纸。

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
  annotations:
    kubernetes.io/change-cause: v1, ngx=1.21
... ...
```

deployment spec 下
minReadySeconds: 15 # 确认Pod就绪的等待时间

在 Deployment 里还有其他一些字段可以对滚动更新的过程做更细致的控制，它们都在 spec.strategy.rollingUpdate 里，比如 maxSurge、maxUnavailable 等字段，分别控制最多新增 Pod 数和最多不可用 Pod 数，一般用默认值就足够了，你如果感兴趣也可以查看 Kubernetes 文档进一步研究。

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ngx-conf

data:
  default.conf: |
    server {
      listen 80;
      location / {
        default_type text/plain;
        return 200
          'ver : $nginx_version\nsrv : $server_addr:$server_port\nhost: $hostname\n';
      }
    }

```

* kubectl apply
* kubectl edit
* kubectl patch
* kubectl set image

不会记录所有的更新历史，默认保留最近10次操作，可以通过 revisonHistoryLimit 修改

# pod 更健康

* 创建容器有三大隔离技术：namespace、cgroup、chroot。
* namespace 实现了独立的进程空间，
* chroot 实现了独立的文件系统，
* cgroup 的作用是管控 CPU、内存，保证容器不会无节制地占用基础资源，进而影响到系统里的其他应用。

重点学习的是 containers.resources，它下面有两个字段：
* “requests”，意思是容器要申请的资源，也就是说要求 Kubernetes 在创建 Pod 的时候必须分配这里列出的资源，否则容器就无法运行。
* “limits”，意思是容器使用资源的上限，不能超过设定值，否则就有可能被强制停止运行。
* 不过 CPU 时间也不能无限分割，Kubernetes 里 CPU 的最小使用单位是 0.001，为了方便表示用了一个特别的单位 m，也就是“milli”“毫”的意思，比如说 500m 就相当于 0.5。
* 如果 Pod 不写 resources 字段，意味着 Pod 对运行的资源要求“既没有下限，也没有上限”，Kubernetes 不用管 CPU 和内存是否足够，可以把 Pod 调度到任意的节点上，而且后续 Pod 运行时也可以无限制地使用 CPU 和内存。

# “探针”（Probe）

Kubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态： 
* Startup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。 
* Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。 
* Readiness，就绪探针，用来检查应用是否可以接收流量，是否能够对外提供服务。

三种探针的配置方式都是一样的，关键字段有这么几个： 
* periodSeconds，执行探测动作的时间间隔，默认是 10 秒探测一次。 
* timeoutSeconds，探测动作的超时时间，如果超时就认为探测失败，默认是 1 秒。 
* successThreshold，连续几次探测成功才认为是正常，对于 startupProbe 和 livenessProbe 来说它只能是 1。 failureThreshold，连续探测失败几次才认为是真正发生了异常，默认是 3 次。

探测方式，Kubernetes 支持 3 种：Shell、TCP Socket、HTTP GET，它们也需要在探针里配置： 
* exec，执行一个 Linux 命令，比如 ps、cat 等等，和 container 的 command 字段很类似。 
* tcpSocket，使用 TCP 协议尝试连接容器的指定端口。 
* httpGet，连接端口并发送 HTTP GET 请求。

`kc logs  ngx-pod-probe`

* StartupProbe 和 LivenessProbe 探测失败后的动作其实是由字段 restartPolicy 决定的，它的默认值 On-Failure 就是重启容器
* initialDelaySeconds 容器启动多久之后再探测，默认是0
* gRPC 探测目前是beta版本
* lifecycle 字段 ，可以在启动后中终止前安装两个钩子 postStart preStop   

# ResourceQuota

```sh
export out="--dry-run=client -o yaml"
kubectl create quota dev-qt $out

# 资源配额对象必须依附在某个名字空间

kubectl describe quota -n dev-ns

kubectl create job echo1 -n dev-ns --image=busybox -- echo hello
kubectl create job echo2 -n dev-ns --image=busybox -- echo hello

kubectl run ngx --image=nginx:alpine -n dev-ns
# Error from server (Forbidden): pods "ngx" is forbidden: failed quota: dev-qt: must specify limits.cpu,limits.memory,requests.cpu,requests.memory

kubectl describe limitranges -n dev-ns

# 不是所有的API对象都可以划分到ns里，比如 node pv 这样的全局对象不属于任何空间
# ResourceQuota 可以使用 scopeSelector 字段限制不同类型的对象，所以我们还可以在名字空间里设置多个不同策略的配额对象，更精细地控制资源

```

# monitor

Kubernetes 为集群提供的两种系统级别的监控项目：Metrics Server 和 Prometheus，以及基于它们的水平自动伸缩对象 HorizontalPodAutoscaler。

```sh
wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# add --kubelet-insecure-tls
# pull image

kubectl get pod -n kube-system

kubectl top node
kubectl top pod -n kube-system

```

* metric server 早期数据来源是 cAdvisor 它是一个独立的应用程序 ，后来被精简集成进了 kubelet 
* metric server 里的pod 里应用了资源配额和检查探针
* 当前 Hpa  版本是v2，除了支持CPU指标，也支持自定义指标（比如RPS）,还有更多的可调节参数，但命令 kubectl autoscale 创建的yaml默认用的是v1
* 

# HorizontalPodAutoscaler

* 注意在它的 spec 里一定要用 resources 字段写清楚资源配额，否则 HorizontalPodAutoscaler 会无法获取 Pod 的指标，也就无法实现自动化扩缩容。
* HorizontalPodAutoscaler 实现了应用的自动水平伸缩功能，它从 Metrics Server 获取应用的运行指标，再实时调整 Pod 数量，可以很好地应对突发流量。

```sh
kubectl autoscale deploy ngx-hpa-dep --min=2 --max=10 --cpu-percent=5 $out

# 里面有ab工具
kubectl run test -it --image=httpd:alpine -- sh

ab -c 10 -t 60 -n 1000000 'http://ngx-hpa-svc/'

kc get hpa ngx-hpa-dep -w
```

# Prometheus

* Prometheus 系统的核心是它的 Server，里面有一个时序数据库 TSDB，用来存储监控数据，另一个组件 Retrieval 使用拉取（Pull）的方式从各个目标收集数据，再通过 HTTP Server 把这些数据交给外界使用。 
* 在 Prometheus Server 之外还有三个重要的组件： 
    * Push Gateway，用来适配一些特殊的监控目标，把默认的 Pull 模式转变为 Push 模式。 
    * Alert Manager，告警中心，预先设定规则，发现问题时就通过邮件等方式告警。 
    * Grafana 是图形化界面，可以定制大量直观的监控仪表盘。
* https://github.com/prometheus-operator/kube-prometheus/
* `wget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.11.0.tar.gz`
* 和 Metrics Server 一样，我们也必须要做一些准备工作，才能够安装 Prometheus。 
    * 第一步，是修改 prometheus-service.yaml、grafana-service.yaml。 这两个文件定义了 Prometheus 和 Grafana 服务对象，我们可以给它们添加 type: NodePort，这样就可以直接通过节点的 IP 地址访问（当然你也可以配置成 Ingress）。
    * 第二步，是修改 kubeStateMetrics-deployment.yaml、prometheusAdapter-deployment.yaml，因为它们里面有两个存放在 gcr.io 的镜像，必须解决下载镜像的问题。 但很遗憾，我没有在国内网站上找到它们的下载方式，为了能够顺利安装，只能把它们下载后再上传到 Docker Hub 上。所以你需要修改镜像名字，把前缀都改成 chronolaw： 
    ```
    image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.5.0
    image: k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1
    
    image: chronolaw/kube-state-metrics:v2.5.0
    image: chronolaw/prometheus-adapter:v0.9.1
    ```
    
    * 这两个准备工作完成之后，我们要执行两个 kubectl create 命令来部署 Prometheus，先是 manifests/setup 目录，创建名字空间等基本对象，然后才是 manifests 目录：
    ```
    kubectl create -f manifests/setup
    kubectl create -f manifests
    ```
    * Prometheus 的对象都在名字空间“monitoring”里
    * Prometheus 的 Web 界面比较简单，通常只用来调试、测试，不适合实际监控。我们再来看 Grafana，访问节点的端口“30358”，它会要求你先登录，默认的用户名和密码都是“admin”：
    * Prometheus 是云原生监控领域的“事实标准”，用 PromQL 语言来查询数据，配合 Grafana 可以展示直观的图形界面，方便监控。

# Dashboard

* https://github.com/kubernetes/dashboard
* `wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml`
* `kubectl apply -f dashboard.yaml`

```sh
openssl req -x509 -days 365 -out k8s.test.crt -keyout k8s.test.key \
  -newkey rsa:2048 -nodes -sha256 \
    -subj '/CN=k8s.test' -extensions EXT -config <( \
       printf "[dn]\nCN=k8s.test\n[req]\ndistinguished_name = dn\n[EXT]\nsubjectAltName=DNS:k8s.test\nkeyUsage=digitalSignature\nextendedKeyUsage=serverAuth")

export out="--dry-run=client -o yaml"
kubectl create secret tls dash-tls -n kubernetes-dashboard --cert=k8s.test.crt --key=k8s.test.key $out > cert.yml

kubectl create ing dash-ing --rule="k8s.test/=kubernetes-dashboard:443" --class=dash-ink -n kubernetes-dashboard $out



```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dash-kic-dep
  namespace: nginx-ingress

spec:
  ...
        args:
          - -ingress-class=dash-ink

```

* https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md
* RBAC Role-Based Access Control

# 网络接口标准 CNI(Container Networking Interface)


* 依据实现技术的不同，CNI 插件可以大致上分成“Overlay”“Route”和“Underlay”三种。
* Overlay 的原意是“覆盖”，是指它构建了一个工作在真实底层网络之上的“逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。 
* Route 也是在底层网络之上工作，但它没有封包和拆包，而是使用系统内置的路由功能来实现 Pod 跨主机通信。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了。 
* Underlay 就是直接用底层网络来实现 CNI，也就是说 Pod 和宿主机都在一个网络里，Pod 和宿主机是平等的。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。
* Flannel
    * https://github.com/flannel-io/flannel/
    * 由 CoreOS 公司（已被 Redhat 收购）开发，
    * 最早是一种 Overlay 模式的网络插件，使用 UDP 和 VXLAN 技术，后来又用 Host-Gateway 技术支持了 Route 模式。
    * Flannel 简单易用，是 Kubernetes 里最流行的 CNI 插件，但它在性能方面表现不是太好，所以一般不建议在生产环境里使用。
* Calico
    * https://github.com/projectcalico/calico
    * 是一种 Route 模式的网络插件，使用 BGP 协议（Border Gateway Protocol）来维护路由信息，性能要比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能。
    * `wget https://projectcalico.docs.tigera.io/manifests/calico.yaml`
    * 由于 Calico 使用的镜像较大，为了加快安装速度，可以考虑在每个节点上预先使用 docker pull 拉取镜像
    ```sh
    docker pull calico/cni:v3.23.1
    docker pull calico/node:v3.23.1
    docker pull calico/kube-controllers:v3.23.1

    ```
    * `kubectl apply -f calico.yaml`


* Cilium
    * https://github.com/cilium/cilium
    * 是一个比较新的网络插件，
    * 同时支持 Overlay 模式和 Route 模式，
    * 它的特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，所以性能很高，可以灵活实现各种功能。
    * 在 2021 年它加入了 CNCF，成为了孵化项目，是非常有前途的 CNI 插件。

```sh
ip addr
brctl show
route
ip neighbor
bridge fdb
```

* docker 曾经提过 CNM(Container Network Model)，但是竞争不过背靠 k8s和CNCF的CNI，失败了
* CNI 的可执行文件都存放在节点  /opt/cni/bin 目录里，Flannel 的子网配置文件是 /run/channel/subnet.env
* tshark

# next study
* https://kubernetes.io/zh-cn/docs/home/
* https://kubernetes.io/blog/
* https://www.cncf.io/
* CNCF 全景图里的项目非常多，其中由它托管的项目又分成
    * 毕业（Graduated）项目
    * 孵化（Incubating）项目
    * 沙盒（Sandbox）项目
* https://kubernetes.io/zh-cn/training/


https://time.geekbang.org/quiz/?act_id=4661&exam_id=10696&source=2857435
