

# docker-compose

```sh
docker-compose -f wp-compose.yml up -d
docker-compose ps
docker-compose -f wp-compose.yml exec -it nginx sh

# ping mariadb 和 wordpress 这两个服务，网络都是通的，不过它的 IP 地址段用的是“172.22.0.0/16”，和 Docker 默认的“172.17.0.0/16”不一样。
# docker-compose 的命令与 Docker 类似，比较常用的有 up、ps、down，用来启动、查看和停止应用。
# 默认名字是 composer.yml 如果是这个名字，就不用加 -f 了
```

# PersistentVolume

“accessModes”定义了存储设备的访问模式，简单来说就是虚拟盘的读写权限，和 Linux 的文件访问模式差不多，目前 Kubernetes 里有 3 种： 
* ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。
* ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。 
* ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。

Kubernetes 里定义存储容量使用的是国际标准，我们日常习惯使用的 KB/MB/GB 的基数是 1024，要写成 Ki/Mi/Gi

kubectl get pv
kubectl get pvc

## NFS

```sh
sudo apt -y install nfs-kernel-server
sudo yum -y install nfs-utils rpcbind 
mkdir -p /tmp/nfs

`/etc/exports`

/tmp/nfs 10.0.2.0/24(rw,sync,no_subtree_check,no_root_squash,insecure)

sudo exportfs -ra
sudo exportfs -v

sudo systemctl start  nfs-server
sudo systemctl enable nfs-server
sudo systemctl status nfs-server

showmount -e 127.0.0.1

# 有了 NFS 服务器之后，为了让 Kubernetes 集群能够访问 NFS 存储服务，我们还需要在每个节点上都安装 NFS 客户端。
sudo apt -y install nfs-common

showmount -e 10.0.2.4
mkdir -p /tmp/test
sudo mount -t nfs 10.0.2.4:/tmp/nfs /tmp/test


```

## dynamic

* 这个在 Kubernetes 里就是“动态存储卷”的概念，它可以用 StorageClass 绑定一个 Provisioner 对象，
* 而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。 
* 有了“动态存储卷”的概念，前面我们讲的手工创建的 PV 就可以称为“静态存储卷”。

https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

* NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。

* 不过这三个文件只是示例，想在我们的集群里真正运行起来还要修改其中的两个文件。 
* 第一个要修改的是 rbac.yaml，它使用的是默认的 default 名字空间，应该把它改成其他的名字空间，避免与普通应用混在一起，你可以用“查找替换”的方式把它统一改成 kube-system。 
* 第二个要修改的是 deployment.yaml，它要修改的地方比较多。首先要把名字空间改成和 rbac.yaml 一样，比如是 kube-system，然后重点要修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。

provisioner.go:247] Error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

# StatefulSet

* 启动顺序
* 依赖关系
* 网络标识

```sh
kubectl get sts

kubectl exec -it redis-sts-0 -- sh

echo $HOSTNAME
hostname
```

* 名字顺序
* 启动顺序

# rollout

```sh
kubectl rollout status deployment ngx-dep
kubectl rollout pause
kubectl rollout resume
kubectl rollout history deploy ngx-dep
kubectl rollout history deploy ngx-dep  --revision=2
kubectl rollout undo deploy ngx-dep
```

如果用一个简单的比喻来说呢，annotations 就是包装盒里的产品说明书，而 labels 是包装盒外的标签贴纸。

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ngx-dep
  annotations:
    kubernetes.io/change-cause: v1, ngx=1.21
... ...
```

deployment spec 下
minReadySeconds: 15 # 确认Pod就绪的等待时间

在 Deployment 里还有其他一些字段可以对滚动更新的过程做更细致的控制，它们都在 spec.strategy.rollingUpdate 里，比如 maxSurge、maxUnavailable 等字段，分别控制最多新增 Pod 数和最多不可用 Pod 数，一般用默认值就足够了，你如果感兴趣也可以查看 Kubernetes 文档进一步研究。

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ngx-conf

data:
  default.conf: |
    server {
      listen 80;
      location / {
        default_type text/plain;
        return 200
          'ver : $nginx_version\nsrv : $server_addr:$server_port\nhost: $hostname\n';
      }
    }

```

* kubectl apply
* kubectl edit
* kubectl patch
* kubectl set image

不会记录所有的更新历史，默认保留最近10次操作，可以通过 revisonHistoryLimit 修改

# pod 更健康

* 创建容器有三大隔离技术：namespace、cgroup、chroot。
* namespace 实现了独立的进程空间，
* chroot 实现了独立的文件系统，
* cgroup 的作用是管控 CPU、内存，保证容器不会无节制地占用基础资源，进而影响到系统里的其他应用。

重点学习的是 containers.resources，它下面有两个字段：
* “requests”，意思是容器要申请的资源，也就是说要求 Kubernetes 在创建 Pod 的时候必须分配这里列出的资源，否则容器就无法运行。
* “limits”，意思是容器使用资源的上限，不能超过设定值，否则就有可能被强制停止运行。
* 不过 CPU 时间也不能无限分割，Kubernetes 里 CPU 的最小使用单位是 0.001，为了方便表示用了一个特别的单位 m，也就是“milli”“毫”的意思，比如说 500m 就相当于 0.5。
* 如果 Pod 不写 resources 字段，意味着 Pod 对运行的资源要求“既没有下限，也没有上限”，Kubernetes 不用管 CPU 和内存是否足够，可以把 Pod 调度到任意的节点上，而且后续 Pod 运行时也可以无限制地使用 CPU 和内存。

# “探针”（Probe）

Kubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态： 
* Startup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。 
* Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。 
* Readiness，就绪探针，用来检查应用是否可以接收流量，是否能够对外提供服务。

三种探针的配置方式都是一样的，关键字段有这么几个： 
* periodSeconds，执行探测动作的时间间隔，默认是 10 秒探测一次。 
* timeoutSeconds，探测动作的超时时间，如果超时就认为探测失败，默认是 1 秒。 
* successThreshold，连续几次探测成功才认为是正常，对于 startupProbe 和 livenessProbe 来说它只能是 1。 failureThreshold，连续探测失败几次才认为是真正发生了异常，默认是 3 次。

探测方式，Kubernetes 支持 3 种：Shell、TCP Socket、HTTP GET，它们也需要在探针里配置： 
* exec，执行一个 Linux 命令，比如 ps、cat 等等，和 container 的 command 字段很类似。 
* tcpSocket，使用 TCP 协议尝试连接容器的指定端口。 
* httpGet，连接端口并发送 HTTP GET 请求。

`kc logs  ngx-pod-probe`

* StartupProbe 和 LivenessProbe 探测失败后的动作其实是由字段 restartPolicy 决定的，它的默认值 On-Failure 就是重启容器
* initialDelaySeconds 容器启动多久之后再探测，默认是0
* gRPC 探测目前是beta版本
* lifecycle 字段 ，可以在启动后中终止前安装两个钩子 postStart preStop   

# ResourceQuota

```sh
export out="--dry-run=client -o yaml"
kubectl create quota dev-qt $out

# 资源配额对象必须依附在某个名字空间

kubectl describe quota -n dev-ns

kubectl create job echo1 -n dev-ns --image=busybox -- echo hello
kubectl create job echo2 -n dev-ns --image=busybox -- echo hello

kubectl run ngx --image=nginx:alpine -n dev-ns
# Error from server (Forbidden): pods "ngx" is forbidden: failed quota: dev-qt: must specify limits.cpu,limits.memory,requests.cpu,requests.memory

kubectl describe limitranges -n dev-ns

# 不是所有的API对象都可以划分到ns里，比如 node pv 这样的全局对象不属于任何空间
# ResourceQuota 可以使用 scopeSelector 字段限制不同类型的对象，所以我们还可以在名字空间里设置多个不同策略的配额对象，更精细地控制资源

```